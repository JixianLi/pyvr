# Phase 02: Testing and Validation

**Feature:** v0.3.3 Ray Marching Opacity Consistency
**Phase:** 02 of 03
**Focus:** Comprehensive testing of opacity correction implementation

## Scope

Add comprehensive tests to validate opacity correction:
- Unit tests for correction formula mathematics
- Tests for RenderConfig parameter behavior
- Visual comparison validation across quality presets
- Regression tests to ensure existing functionality works
- Edge case testing (alpha=0, alpha=1, extreme step sizes)

This phase ensures the implementation is correct, robust, and doesn't break existing functionality.

## Implementation

### Task 1: Create Unit Tests for Opacity Correction Formula

**File:** `tests/test_config_opacity_correction.py` (NEW FILE)

**Create complete test file:**

```python
"""Tests for opacity correction formula and RenderConfig integration."""

import pytest
import numpy as np
from pyvr.config import RenderConfig


class TestReferenceStepSizeParameter:
    """Tests for reference_step_size parameter in RenderConfig."""

    def test_reference_step_size_default(self):
        """Test that reference_step_size defaults to 0.01."""
        config = RenderConfig.balanced()
        assert config.reference_step_size == 0.01

    def test_reference_step_size_all_presets(self):
        """Test that all presets use default reference_step_size."""
        presets = [
            RenderConfig.preview(),
            RenderConfig.fast(),
            RenderConfig.balanced(),
            RenderConfig.high_quality(),
            RenderConfig.ultra_quality(),
        ]

        for preset in presets:
            assert preset.reference_step_size == 0.01

    def test_reference_step_size_custom(self):
        """Test custom reference_step_size value."""
        config = RenderConfig(
            step_size=0.01,
            max_steps=500,
            reference_step_size=0.005
        )
        assert config.reference_step_size == 0.005

    def test_reference_step_size_preserved_with_step_size(self):
        """Test that with_step_size() preserves reference_step_size."""
        config = RenderConfig.balanced().with_step_size(0.008)
        assert config.reference_step_size == 0.01  # Should preserve default

    def test_reference_step_size_preserved_with_max_steps(self):
        """Test that with_max_steps() preserves reference_step_size."""
        config = RenderConfig.balanced().with_max_steps(600)
        assert config.reference_step_size == 0.01  # Should preserve default


class TestOpacityCorrectionFormula:
    """Tests for opacity correction mathematical formula."""

    def test_formula_identity_when_equal_step_sizes(self):
        """When step_size == reference_step_size, correction should be minimal."""
        alpha_tf = 0.5
        step_size = 0.01
        reference = 0.01

        # Formula: 1.0 - exp(-alpha_tf * step_size / reference)
        corrected = 1.0 - np.exp(-alpha_tf * step_size / reference)

        # When step == reference, corrected should be close to original
        # (not exactly equal due to exponential, but close for small alpha)
        assert np.isclose(corrected, alpha_tf, rtol=0.1)

    def test_smaller_step_produces_smaller_alpha(self):
        """Smaller step size should produce smaller corrected alpha."""
        alpha_tf = 0.5
        reference = 0.01

        corrected_small = 1.0 - np.exp(-alpha_tf * 0.005 / reference)
        corrected_large = 1.0 - np.exp(-alpha_tf * 0.02 / reference)

        assert corrected_small < corrected_large

    def test_larger_step_produces_larger_alpha(self):
        """Larger step size should produce larger corrected alpha."""
        alpha_tf = 0.3
        reference = 0.01

        corrected_preview = 1.0 - np.exp(-alpha_tf * 0.05 / reference)  # preview
        corrected_balanced = 1.0 - np.exp(-alpha_tf * 0.01 / reference)  # balanced

        assert corrected_preview > corrected_balanced

    def test_alpha_zero_stays_zero(self):
        """Alpha of 0 should remain 0 after correction."""
        reference = 0.01
        step = 0.005

        corrected = 1.0 - np.exp(-0.0 * step / reference)
        assert corrected == 0.0

    def test_alpha_one_behavior(self):
        """Alpha of 1.0 should produce high but valid corrected value."""
        reference = 0.01
        step = 0.005

        corrected = 1.0 - np.exp(-1.0 * step / reference)

        # Should be positive but less than 1
        assert 0.0 < corrected < 1.0

    def test_correction_monotonic_increasing(self):
        """Corrected alpha should increase monotonically with step_size."""
        alpha_tf = 0.4
        reference = 0.01

        step_sizes = [0.001, 0.005, 0.01, 0.02, 0.05]
        corrected_values = [
            1.0 - np.exp(-alpha_tf * step / reference)
            for step in step_sizes
        ]

        # Each value should be greater than or equal to previous
        for i in range(1, len(corrected_values)):
            assert corrected_values[i] >= corrected_values[i-1]

    def test_correction_bounds(self):
        """Corrected alpha should always be in [0, 1) range."""
        alpha_values = np.linspace(0.0, 1.0, 20)
        step_sizes = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1]
        reference = 0.01

        for alpha_tf in alpha_values:
            for step in step_sizes:
                corrected = 1.0 - np.exp(-alpha_tf * step / reference)

                # Should be in valid range
                assert 0.0 <= corrected < 1.0

    def test_extreme_step_size_small(self):
        """Very small step size should produce very small corrected alpha."""
        alpha_tf = 0.5
        reference = 0.01
        step_tiny = 0.0001

        corrected = 1.0 - np.exp(-alpha_tf * step_tiny / reference)

        # Should be much smaller than original alpha
        assert corrected < alpha_tf * 0.1

    def test_extreme_step_size_large(self):
        """Very large step size should saturate near 1.0."""
        alpha_tf = 0.5
        reference = 0.01
        step_huge = 1.0

        corrected = 1.0 - np.exp(-alpha_tf * step_huge / reference)

        # Should be close to 1.0 but not exceed it
        assert 0.99 < corrected < 1.0

    def test_reference_step_size_scaling(self):
        """Different reference_step_size should scale correction appropriately."""
        alpha_tf = 0.5
        step = 0.01

        # Using step as reference should give minimal correction
        corrected_ref_equal = 1.0 - np.exp(-alpha_tf * step / step)

        # Using smaller reference should increase correction
        corrected_ref_small = 1.0 - np.exp(-alpha_tf * step / 0.005)

        # Using larger reference should decrease correction
        corrected_ref_large = 1.0 - np.exp(-alpha_tf * step / 0.02)

        assert corrected_ref_small > corrected_ref_equal > corrected_ref_large


class TestOpacityCorrectionPhysics:
    """Tests for physical correctness of opacity correction."""

    def test_beer_lambert_law(self):
        """Verify formula matches Beer-Lambert law: I = Iâ‚€ * exp(-Ï„*d)."""
        # For opacity: alpha = 1 - exp(-tau * distance)
        # In our case: tau = alpha_tf / reference_step_size
        #              distance = step_size

        alpha_tf = 0.6  # Extinction coefficient (scaled)
        step = 0.01
        reference = 0.01

        # Our formula
        alpha_corrected = 1.0 - np.exp(-alpha_tf * step / reference)

        # Beer-Lambert: transmittance = exp(-tau * d)
        # Therefore: opacity = 1 - transmittance
        tau = alpha_tf / reference
        distance = step
        opacity_beer_lambert = 1.0 - np.exp(-tau * distance)

        assert np.isclose(alpha_corrected, opacity_beer_lambert)

    def test_accumulation_consistency(self):
        """Two small steps should approximately equal one large step."""
        alpha_tf = 0.4
        reference = 0.01

        # One step of size 0.02
        alpha_single = 1.0 - np.exp(-alpha_tf * 0.02 / reference)

        # Two steps of size 0.01
        alpha_step1 = 1.0 - np.exp(-alpha_tf * 0.01 / reference)
        # After first step, remaining transmittance is (1 - alpha_step1)
        # Second step adds more opacity
        alpha_step2 = 1.0 - np.exp(-alpha_tf * 0.01 / reference)
        # Combined using front-to-back blending
        alpha_combined = alpha_step1 + (1.0 - alpha_step1) * alpha_step2

        # Should be close (not exact due to discrete steps)
        assert np.isclose(alpha_single, alpha_combined, rtol=0.05)
```

**Verification:**
```bash
pytest tests/test_config_opacity_correction.py -v
```

Expected: All tests pass (approximately 20+ tests).

### Task 2: Add Renderer Integration Tests

**File:** `tests/test_moderngl_renderer/test_opacity_correction_integration.py` (NEW FILE)

**Create integration test file:**

```python
"""Integration tests for opacity correction in VolumeRenderer."""

import pytest
import numpy as np
from pyvr.moderngl_renderer import VolumeRenderer
from pyvr.config import RenderConfig
from pyvr.volume import Volume
from pyvr.datasets import create_sample_volume
from pyvr.transferfunctions import ColorTransferFunction, OpacityTransferFunction


@pytest.fixture
def test_volume():
    """Create a test volume."""
    volume_data = create_sample_volume(64, 'sphere')
    return Volume(data=volume_data)


@pytest.fixture
def test_renderer(test_volume):
    """Create a renderer with test volume loaded."""
    renderer = VolumeRenderer(width=256, height=256)
    renderer.load_volume(test_volume)

    # Set transfer functions
    ctf = ColorTransferFunction.from_colormap('viridis')
    otf = OpacityTransferFunction.linear(0.0, 0.5)
    renderer.set_transfer_functions(ctf, otf)

    return renderer


class TestOpacityCorrectionIntegration:
    """Integration tests for opacity correction in rendering pipeline."""

    def test_reference_step_size_uniform_exists(self, test_renderer):
        """Test that reference_step_size uniform exists in shader."""
        assert 'reference_step_size' in test_renderer.program

    def test_reference_step_size_uniform_value(self, test_renderer):
        """Test that reference_step_size uniform has correct value."""
        config = RenderConfig.balanced()
        test_renderer.set_config(config)

        # Uniform should match config
        uniform_value = test_renderer.program['reference_step_size'].value
        assert uniform_value == config.reference_step_size

    def test_custom_reference_step_size(self, test_renderer):
        """Test setting custom reference_step_size."""
        config = RenderConfig(
            step_size=0.01,
            max_steps=500,
            reference_step_size=0.008
        )
        test_renderer.set_config(config)

        uniform_value = test_renderer.program['reference_step_size'].value
        assert uniform_value == 0.008

    def test_all_presets_render_successfully(self, test_renderer):
        """Test that all presets render without errors."""
        presets = ['preview', 'fast', 'balanced', 'high_quality', 'ultra_quality']

        for preset_name in presets:
            preset_method = getattr(RenderConfig, preset_name)
            config = preset_method()
            test_renderer.set_config(config)

            # Should render without errors
            data = test_renderer.render()
            assert data is not None

            # Verify image is valid
            image = np.frombuffer(data, dtype=np.uint8).reshape((256, 256, 4))
            assert image.shape == (256, 256, 4)
            assert image.max() > 0  # Not all black

    def test_opacity_consistency_across_presets(self, test_renderer):
        """Test that different presets produce similar overall opacity."""
        presets_to_test = ['preview', 'balanced', 'high_quality']
        mean_alphas = []

        for preset_name in presets_to_test:
            preset_method = getattr(RenderConfig, preset_name)
            config = preset_method()
            test_renderer.set_config(config)

            data = test_renderer.render()
            image = np.frombuffer(data, dtype=np.uint8).reshape((256, 256, 4))

            mean_alpha = image[:,:,3].mean()
            mean_alphas.append(mean_alpha)

        # All mean alphas should be within reasonable range of each other
        # (not identical due to sampling differences, but similar)
        mean_alpha_overall = np.mean(mean_alphas)
        for mean_alpha in mean_alphas:
            # Within 50% of overall mean (generous tolerance for sampling differences)
            assert abs(mean_alpha - mean_alpha_overall) / mean_alpha_overall < 0.5

    def test_no_fully_transparent_renders(self, test_renderer):
        """Test that no preset produces fully transparent render."""
        presets = ['preview', 'fast', 'balanced', 'high_quality', 'ultra_quality']

        for preset_name in presets:
            preset_method = getattr(RenderConfig, preset_name)
            test_renderer.set_config(preset_method())

            data = test_renderer.render()
            image = np.frombuffer(data, dtype=np.uint8).reshape((256, 256, 4))

            # Should have some opacity
            assert image[:,:,3].max() > 0, f"{preset_name} should not be fully transparent"

    def test_different_reference_step_sizes(self, test_renderer):
        """Test rendering with different reference_step_size values."""
        reference_values = [0.005, 0.01, 0.015, 0.02]

        for ref_step in reference_values:
            config = RenderConfig(
                step_size=0.01,
                max_steps=500,
                reference_step_size=ref_step
            )
            test_renderer.set_config(config)

            # Should render successfully
            data = test_renderer.render()
            assert data is not None

            image = np.frombuffer(data, dtype=np.uint8).reshape((256, 256, 4))
            assert image.max() > 0


class TestOpacityCorrectionRegression:
    """Regression tests to ensure opacity correction doesn't break existing functionality."""

    def test_existing_rendering_still_works(self, test_renderer):
        """Test that existing rendering functionality is not broken."""
        # This is the "smoke test" - basic rendering should still work
        data = test_renderer.render()
        assert data is not None

        image = np.frombuffer(data, dtype=np.uint8).reshape((256, 256, 4))
        assert image.shape == (256, 256, 4)

    def test_transfer_function_changes_still_work(self, test_renderer):
        """Test that changing transfer functions still works."""
        # Change color map
        ctf_new = ColorTransferFunction.from_colormap('plasma')
        otf = OpacityTransferFunction.linear(0.0, 0.3)
        test_renderer.set_transfer_functions(ctf_new, otf)

        data = test_renderer.render()
        assert data is not None

    def test_camera_changes_still_work(self, test_renderer):
        """Test that camera changes still work."""
        from pyvr.camera import Camera

        camera = Camera.front_view(distance=4.0)
        test_renderer.set_camera(camera)

        data = test_renderer.render()
        assert data is not None

    def test_light_changes_still_work(self, test_renderer):
        """Test that light changes still work."""
        from pyvr.lighting import Light

        light = Light.directional([1, -1, -1], ambient=0.3, diffuse=0.7)
        test_renderer.set_light(light)

        data = test_renderer.render()
        assert data is not None
```

**Verification:**
```bash
pytest tests/test_moderngl_renderer/test_opacity_correction_integration.py -v
```

Expected: All integration tests pass.

### Task 3: Visual Comparison Test Script

**File:** `tests/visual_comparison_opacity_correction.py` (NEW FILE, not in test suite)

**Purpose:** Manual visual validation tool for comparing presets.

```python
"""Visual comparison tool for opacity correction across quality presets.

This script renders a test volume at all quality presets and saves images
for visual comparison. Use this to verify that all presets produce similar
overall opacity/brightness.

Usage:
    python tests/visual_comparison_opacity_correction.py

Output:
    - opacity_comparison_preview.png
    - opacity_comparison_fast.png
    - opacity_comparison_balanced.png
    - opacity_comparison_high_quality.png
    - opacity_comparison_ultra_quality.png
"""

from pyvr.moderngl_renderer import VolumeRenderer
from pyvr.config import RenderConfig
from pyvr.volume import Volume
from pyvr.datasets import create_sample_volume
from pyvr.transferfunctions import ColorTransferFunction, OpacityTransferFunction
from PIL import Image
import numpy as np


def render_all_presets():
    """Render test volume at all presets for visual comparison."""
    print("Creating test volume...")
    volume_data = create_sample_volume(128, 'double_sphere')
    volume = Volume(data=volume_data)

    print("Setting up renderer...")
    renderer = VolumeRenderer(width=512, height=512)
    renderer.load_volume(volume)

    # Use same transfer functions for all
    ctf = ColorTransferFunction.from_colormap('viridis')
    otf = OpacityTransferFunction.linear(0.0, 0.3)
    renderer.set_transfer_functions(ctf, otf)

    presets = ['preview', 'fast', 'balanced', 'high_quality', 'ultra_quality']

    print("\nRendering at all presets...")
    stats = []

    for preset_name in presets:
        print(f"\n{preset_name}:")

        # Get and apply preset
        preset_method = getattr(RenderConfig, preset_name)
        config = preset_method()
        renderer.set_config(config)

        print(f"  step_size: {config.step_size}")
        print(f"  reference_step_size: {config.reference_step_size}")

        # Render
        image = renderer.render_to_pil()

        # Convert to numpy for stats
        img_array = np.array(image)

        # Compute statistics
        mean_alpha = img_array[:,:,3].mean()
        max_alpha = img_array[:,:,3].max()
        mean_rgb = img_array[:,:,:3].mean()

        print(f"  Mean alpha: {mean_alpha:.1f}")
        print(f"  Max alpha: {max_alpha}")
        print(f"  Mean RGB: {mean_rgb:.1f}")

        stats.append({
            'preset': preset_name,
            'mean_alpha': mean_alpha,
            'max_alpha': max_alpha,
            'mean_rgb': mean_rgb
        })

        # Save image
        filename = f'opacity_comparison_{preset_name}.png'
        image.save(filename)
        print(f"  Saved: {filename}")

    # Print comparison summary
    print("\n" + "="*60)
    print("COMPARISON SUMMARY")
    print("="*60)
    print(f"{'Preset':<20} {'Mean Alpha':>12} {'Max Alpha':>12} {'Mean RGB':>12}")
    print("-"*60)

    for stat in stats:
        print(f"{stat['preset']:<20} {stat['mean_alpha']:>12.1f} "
              f"{stat['max_alpha']:>12} {stat['mean_rgb']:>12.1f}")

    print("\nâœ“ Visual comparison images saved")
    print("\nManual Verification:")
    print("- All images should have similar overall opacity/brightness")
    print("- Differences should only be in smoothness (sampling quality)")
    print("- No image should be significantly lighter or darker than others")


if __name__ == '__main__':
    render_all_presets()
```

**Verification:**
```bash
python tests/visual_comparison_opacity_correction.py
```

Expected output:
- 5 PNG images saved
- Statistics printed showing similar mean_alpha across presets
- Visual inspection confirms consistent appearance

### Task 4: Update Existing Tests if Needed

**Action:** Run all existing tests to ensure no regressions:

```bash
pytest tests/ -v
```

**Expected:** All existing tests should still pass. If any tests fail:

1. Investigate the failure
2. Determine if it's a legitimate regression or expected visual change
3. Update test expectations if visual change is expected (due to opacity correction)

**Common test updates needed:**
- Tests that check exact rendered output may need tolerance adjustments
- Tests that verify specific alpha values may need updates

**Files to check:**
- `tests/test_moderngl_renderer/test_renderer.py`
- `tests/test_config.py`
- Any tests that verify rendering output

## Verification Steps

### Step 1: Run Unit Tests

```bash
# Run new opacity correction tests
pytest tests/test_config_opacity_correction.py -v

# Should see ~20 tests pass
```

### Step 2: Run Integration Tests

```bash
# Run integration tests
pytest tests/test_moderngl_renderer/test_opacity_correction_integration.py -v

# Should see ~10 tests pass
```

### Step 3: Run Full Test Suite

```bash
# Run all tests to check for regressions
pytest tests/ -v

# All tests should pass
```

### Step 4: Run Visual Comparison

```bash
# Generate visual comparison images
python tests/visual_comparison_opacity_correction.py

# Manually inspect the 5 generated PNG images
# Verify they look similar in overall opacity
```

### Step 5: Test Coverage Check

```bash
# Check test coverage for new code
pytest tests/ --cov=pyvr.config --cov=pyvr.moderngl_renderer --cov-report=term-missing

# coverage for opacity correction should be high (>90%)
```

## Validation

### Test Summary Validation

Create summary script to verify all tests:

```bash
# Run comprehensive test validation
pytest tests/test_config_opacity_correction.py \
       tests/test_moderngl_renderer/test_opacity_correction_integration.py \
       tests/test_config.py \
       tests/test_moderngl_renderer/ \
       -v --tb=short

# Expected: All tests pass, no failures
```

### Edge Case Validation

Ensure edge cases are covered:
- [ ] Alpha = 0.0 (fully transparent)
- [ ] Alpha = 1.0 (fully opaque)
- [ ] Step size = reference_step_size (identity case)
- [ ] Very small step size (0.0001)
- [ ] Very large step size (1.0)
- [ ] Small reference_step_size (0.001)
- [ ] Large reference_step_size (0.1)

All edge cases should be tested in `test_config_opacity_correction.py`.

## Acceptance Criteria

- [ ] Unit test file created: `tests/test_config_opacity_correction.py`
- [ ] Integration test file created: `tests/test_moderngl_renderer/test_opacity_correction_integration.py`
- [ ] Visual comparison script created: `tests/visual_comparison_opacity_correction.py`
- [ ] All unit tests pass (~20 tests)
- [ ] All integration tests pass (~10 tests)
- [ ] All existing tests still pass (no regressions)
- [ ] Visual comparison shows consistent opacity across presets
- [ ] Edge cases tested: alpha=0, alpha=1, extreme step sizes
- [ ] Formula correctness verified: Beer-Lambert law
- [ ] Physical consistency verified: accumulation test
- [ ] Test coverage >90% for new code
- [ ] No test failures in full test suite

## Git Commit

**When all acceptance criteria are met**, commit with:

```bash
git add tests/test_config_opacity_correction.py \
        tests/test_moderngl_renderer/test_opacity_correction_integration.py \
        tests/visual_comparison_opacity_correction.py

git commit -m "$(cat <<'EOF'
test(opacity): Add comprehensive tests for opacity correction

Add unit and integration tests for Beer-Lambert opacity correction:

Unit tests (test_config_opacity_correction.py):
- RenderConfig parameter behavior and defaults
- Mathematical formula correctness
- Edge cases (alpha=0, alpha=1, extreme step sizes)
- Physical correctness (Beer-Lambert law)
- Accumulation consistency

Integration tests (test_opacity_correction_integration.py):
- Shader uniform existence and values
- All presets render successfully
- Opacity consistency across presets
- Regression tests for existing functionality

Visual comparison tool:
- Manual validation script for visual inspection
- Renders all 5 presets for comparison
- Generates statistics and images

All tests pass. Visual comparison confirms consistent opacity
across quality presets with opacity correction enabled.

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
EOF
)"
```

## Notes for Next Phase

Phase 03 will focus on documentation:
- Write comprehensive version notes explaining the bug fix
- Update README.md with opacity correction information
- Update CLAUDE.md with v0.3.3 entry
- Create migration guide for users
- Document breaking visual changes with before/after examples

Testing in Phase 02 confirms the implementation is correct and ready for release documentation.
