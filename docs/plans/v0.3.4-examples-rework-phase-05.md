# Phase 05: Create benchmark.py

## Scope

Create performance benchmark example comparing all quality presets.

**What will be built:**
- `example/benchmark.py` - Performance comparison across quality presets
- ~100 lines of heavily commented code
- Console output with timing table and recommendations

**Dependencies:**
- Phase 01 completed (clean example directory)
- Phase 02-04 completed (can reference structure)

**Files affected:**
- CREATE: `example/benchmark.py`

## Implementation

### Task 1: Create file structure with headers and imports
**Action:** Create the file with ABOUTME headers, docstring, and imports.

**File:** `example/benchmark.py`

**Content:**
```python
# ABOUTME: Performance benchmark comparing all quality presets
# ABOUTME: Tests preview, fast, balanced, high_quality, and ultra_quality configs

"""
Quality Presets Performance Benchmark

This example benchmarks PyVR rendering performance across all quality presets.
It measures timing statistics and provides recommendations for each preset's
best use case.

Quality presets control the step_size and max_steps parameters, which determine
how many samples are taken along each ray. More samples = higher quality but slower.
"""

import time

import numpy as np

from pyvr.camera import Camera
from pyvr.config import RenderConfig
from pyvr.datasets import create_sample_volume, compute_normal_volume
from pyvr.lighting import Light
from pyvr.moderngl_renderer import VolumeRenderer
from pyvr.transferfunctions import ColorTransferFunction, OpacityTransferFunction
from pyvr.volume import Volume

# Benchmark parameters
VOLUME_SIZE = 256  # Larger volume for realistic workload
IMAGE_RES = 512    # Standard output resolution
N_RUNS = 10        # Number of timing runs per preset
```

### Task 2: Implement benchmark function
**Action:** Create function to benchmark a single preset.

**Content to add:**
```python
def benchmark_preset(preset_name, config, volume, camera, light, ctf, otf):
    """
    Benchmark a single quality preset.

    Args:
        preset_name: Name of the preset for display
        config: RenderConfig instance to test
        volume: Volume to render
        camera: Camera configuration
        light: Light configuration
        ctf: Color transfer function
        otf: Opacity transfer function

    Returns:
        Dictionary with timing statistics and configuration details
    """
    # Create fresh renderer for this preset
    renderer = VolumeRenderer(IMAGE_RES, IMAGE_RES, config=config, light=light)
    renderer.load_volume(volume)
    renderer.set_camera(camera)
    renderer.set_transfer_functions(ctf, otf)

    # Warmup: First render is often slower due to GPU initialization
    _ = renderer.render()

    # Benchmark: Run N_RUNS times and collect timings
    times = []
    for _ in range(N_RUNS):
        start = time.perf_counter()
        _ = renderer.render()
        end = time.perf_counter()
        times.append((end - start) * 1000)  # Convert to milliseconds

    # Calculate statistics
    times = np.array(times)
    return {
        'name': preset_name,
        'mean': np.mean(times),
        'std': np.std(times),
        'min': np.min(times),
        'max': np.max(times),
        'fps': 1000 / np.mean(times),
        'config': config
    }
```

### Task 3: Implement main function setup
**Action:** Add volume/camera/TF setup in main function.

**Content to add:**
```python
def main():
    """Run the performance benchmark."""
    print("=" * 60)
    print("PyVR Performance Benchmark")
    print("Quality Presets Comparison")
    print("=" * 60)

    # Step 1: Create volume with normals
    # Using larger volume (256Â³) for realistic performance testing
    print(f"\nCreating {VOLUME_SIZE}Â³ volume...")
    volume_data = create_sample_volume(VOLUME_SIZE, 'double_sphere')
    normals = compute_normal_volume(volume_data)
    volume = Volume(
        data=volume_data,
        normals=normals,
        min_bounds=np.array([-1.0, -1.0, -1.0], dtype=np.float32),
        max_bounds=np.array([1.0, 1.0, 1.0], dtype=np.float32)
    )

    # Step 2: Create camera and light (same for all tests)
    camera = Camera.from_spherical(
        target=np.array([0.0, 0.0, 0.0]),
        distance=3.0,
        azimuth=np.pi / 4,    # 45 degrees
        elevation=np.pi / 6,  # 30 degrees
        roll=0.0
    )
    light = Light.default()

    # Step 3: Create transfer functions (same for all tests)
    ctf = ColorTransferFunction.from_colormap('plasma')
    otf = OpacityTransferFunction.linear(0.0, 0.1)
```

### Task 4: Implement benchmark execution
**Action:** Add preset testing loop.

**Content to add in main():**
```python
    # Step 4: Define presets to benchmark
    # Testing all 5 quality levels from fastest to slowest
    presets = [
        ('Preview', RenderConfig.preview()),
        ('Fast', RenderConfig.fast()),
        ('Balanced', RenderConfig.balanced()),
        ('High Quality', RenderConfig.high_quality()),
        ('Ultra Quality', RenderConfig.ultra_quality())
    ]

    print(f"\nBenchmarking {len(presets)} presets ({N_RUNS} runs each)...")
    print(f"Resolution: {IMAGE_RES}x{IMAGE_RES} pixels\n")

    # Run benchmarks
    results = []
    for preset_name, config in presets:
        print(f"Testing {preset_name}... ", end='', flush=True)
        result = benchmark_preset(
            preset_name, config, volume, camera, light, ctf, otf
        )
        results.append(result)
        print(f"âœ“ {result['mean']:.2f} ms ({result['fps']:.1f} FPS)")
```

### Task 5: Implement results display
**Action:** Add formatted output tables and recommendations.

**Content to add in main():**
```python
    # Step 5: Display detailed results
    print("\n" + "=" * 60)
    print("Timing Results")
    print("=" * 60)
    print(f"{'Preset':<15} {'Mean (ms)':<12} {'Std (ms)':<10} {'FPS':<8} {'Speedup':<10}")
    print("-" * 60)

    # Use Balanced as baseline for speedup comparison
    baseline_time = results[2]['mean']  # Balanced is index 2

    for result in results:
        speedup = baseline_time / result['mean']
        print(
            f"{result['name']:<15} "
            f"{result['mean']:>6.2f} Â± {result['std']:<4.2f}  "
            f"{result['fps']:>6.1f}   "
            f"{speedup:>4.2f}x"
        )

    # Step 6: Display configuration details
    print("\n" + "=" * 60)
    print("Configuration Details")
    print("=" * 60)
    print(f"{'Preset':<15} {'Step Size':<12} {'Max Steps':<12} {'Samples/Ray':<12}")
    print("-" * 60)

    for result in results:
        config = result['config']
        samples = config.estimate_samples_per_ray()
        print(
            f"{result['name']:<15} "
            f"{config.step_size:<12.4f} "
            f"{config.max_steps:<12} "
            f"{samples:<12}"
        )

    # Step 7: Display recommendations
    print("\n" + "=" * 60)
    print("Recommendations")
    print("=" * 60)
    print("âœ“ Preview:      Fastest - use for rapid iteration and testing")
    print("âœ“ Fast:         Interactive - use for real-time exploration")
    print("âœ“ Balanced:     Default - good quality/performance balance")
    print("âœ“ High Quality: Production - use for final renders and screenshots")
    print("âœ“ Ultra Quality: Best - maximum quality for publication")

    print("\n" + "=" * 60)
    print("Benchmark Complete")
    print("=" * 60)
```

### Task 6: Add main guard
**Action:** Add standard Python main guard.

**Content to add:**
```python


if __name__ == "__main__":
    main()
```

## Verification

### Step 1: Verify file exists and syntax
```bash
# Check file exists
ls -la example/benchmark.py

# Check syntax
python -m py_compile example/benchmark.py
```

### Step 2: Run the benchmark
```bash
python example/benchmark.py
```

**Expected output:**
- Console prints progress for each preset
- Timing results table with mean, std, FPS, speedup
- Configuration details table
- Recommendations for each preset
- Preview should be fastest, Ultra should be slowest

## Validation

### Console Output Validation
- [ ] Progress messages during testing
- [ ] Checkmarks (âœ“) for completed presets
- [ ] Timing results table formatted correctly
- [ ] Configuration details table formatted correctly
- [ ] Speedup calculated relative to Balanced preset
- [ ] Recommendations printed clearly
- [ ] No errors or warnings

### Performance Validation
- [ ] Preview preset is fastest (highest FPS)
- [ ] Ultra preset is slowest (lowest FPS)
- [ ] Presets ordered by speed: Preview > Fast > Balanced > High > Ultra
- [ ] FPS values reasonable for system (typically 10-200 FPS range)
- [ ] Standard deviation reasonable (< 20% of mean)

### Code Quality Checks
- [ ] ABOUTME headers present (2 lines)
- [ ] Docstring explains benchmarking purpose
- [ ] Heavy inline comments throughout
- [ ] No version references or temporal language
- [ ] ~100 lines total (Â±15 lines acceptable)
- [ ] Tests all 5 quality presets
- [ ] Includes warmup run before timing

### Integration Testing
```bash
# Should run without errors
python example/benchmark.py

# Should test all 5 presets
# Should display formatted tables
```

## Acceptance Criteria

- [ ] `example/benchmark.py` created
- [ ] File has ABOUTME headers (2 lines)
- [ ] File has complete docstring
- [ ] All imports correct and grouped
- [ ] Constants defined at module level (VOLUME_SIZE, IMAGE_RES, N_RUNS)
- [ ] Heavy inline comments explaining benchmark methodology
- [ ] No version references or temporal language
- [ ] Tests all 5 presets: preview, fast, balanced, high_quality, ultra_quality
- [ ] Includes warmup run before timing
- [ ] Runs N_RUNS (10) iterations per preset
- [ ] Displays timing results table
- [ ] Displays configuration details table
- [ ] Shows speedup relative to balanced preset
- [ ] Includes recommendations for each preset
- [ ] Example runs without errors
- [ ] Code is ~100 lines (Â±15 lines)
- [ ] Print statements show progress

## Git Commit

**Commit message:**
```
feat(examples): Add benchmark.py example

Performance benchmark comparing quality presets:
- Tests all 5 presets (preview â†’ ultra)
- Reports timing statistics (mean, std, FPS, speedup)
- Shows configuration details (step_size, max_steps, samples)
- Provides recommendations for each preset use case
- Uses 256Â³ volume for realistic workload testing

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
```

**Files to commit:**
- `example/benchmark.py`

**Command:**
```bash
git add example/benchmark.py
git commit -m "$(cat <<'EOF'
feat(examples): Add benchmark.py example

Performance benchmark comparing quality presets:
- Tests all 5 presets (preview â†’ ultra)
- Reports timing statistics (mean, std, FPS, speedup)
- Shows configuration details (step_size, max_steps, samples)
- Provides recommendations for each preset use case
- Uses 256Â³ volume for realistic workload testing

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
EOF
)"
```
